{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d5c984-4782-425d-88c7-448df8dc9517",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook walks you through the process of using K-Nearest Neighbors (KNN) to develop a model which if fited to a training data set. Using the model, decision boundaries can be decided and prediction made on what class a sample falls. Principle components analysis (PCA) is conducted alongside the KNN inorder to take into consideration relationships between varaibles when developing the model. \n",
    "\n",
    "Here we are working though a data set which include different features of tumors and class them as either benign or malignant.\n",
    "\n",
    "Follow the instructions and work though the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49fe112-e85c-4be9-a2a1-e112d501e7e7",
   "metadata": {},
   "source": [
    "# To start with...\n",
    "We will need to import the neccessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d25b23-4250-477e-9ec2-0d8126cdeda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)        # to silence future warning messages\n",
    "import numpy as np                                                    # for working with arrays and DataFrames\n",
    "import pandas as pd                                                   # for storing our DataFrames                                                       # to use default of seaborn #take out\n",
    "from sklearn.model_selection import train_test_split                  # to split data into test and training data\n",
    "from sklearn.decomposition import PCA                                 # to conduct the PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier                    # we would be using a K Neightbours model\n",
    "from sklearn.preprocessing import StandardScaler                      # for rescaling each variable\n",
    "from sklearn.pipeline import make_pipeline                            # to make a pipeline we can pass our data through\n",
    "from sklearn.model_selection import GridSearchCV                      # for optimising the hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302fee2b-2664-4f16-b9df-69472b9374ca",
   "metadata": {},
   "source": [
    "# The Data\n",
    "If you would like to import and use your own data set, do no run the next cell below but skip to the next.\n",
    "\n",
    "Remember, your data set must be formated in a CSV file correctly. Your columns should include thoese labled: \"mean radius\", \"mean texture\", \"mean perimeter\", \"mean area\", \"mean smoothness\", \"mean compactness\", \"mean concavity\", \"mean concave points\", \"mean symmetry\", \"mean fractal dimension\". The column representing the $y$ result should only contain values of either 0 (to represent benign diagnoses) or 1 (representing a malignant diagnoses).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae81856e-0a90-475a-9626-cf5632276cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you do not have your own data set to work with\n",
    "\n",
    "# Here we will import a data set from scikit-learn called 'load_breast_cancer'\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# We can now split the data to have all the features grouped together in a dataframe (X) and the y result put in a seperate array.\n",
    "X, y = load_breast_cancer(as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d5422-02af-4355-951b-3a98c0cf37e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you have your own correctly formated data set you would like to import and explore\n",
    "name_data = \" \" # write, between the quote makes, the file path of your data CSV or just the filename, if the file is located in the same directory as this notebook \n",
    "                # A url can also be inputed and used to import your data set\n",
    "\n",
    "data = pd.read_csv(f\"{name_data}.csv\")\n",
    "\n",
    "# We can now split the data to have all the features grouped together in a dataframe (X) and the y result put in a seperate array.\n",
    "X, y = data(as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0393e94f-9a8c-4aa1-89c6-ae4ff1f6fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run this Cell no matter your data source!\n",
    "# We want to use only a subset of the data which includes the mean of the different variables, and not the best and worst of each variables\n",
    "X_subset = X[[\"mean radius\", \"mean texture\", \"mean perimeter\", \"mean area\", \"mean smoothness\", \"mean compactness\", \"mean concavity\", \"mean concave points\", \"mean symmetry\", \"mean fractal dimension\"]]\n",
    "\n",
    "# We can now split the subset data into a test and training subsets randomly\n",
    "train_X_s, test_X_s, train_y_s, test_y_s = train_test_split(X_subset, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39afee4-321a-4100-b22c-f4aa0bf0214c",
   "metadata": {},
   "source": [
    "# Using machine learning to optimise model\n",
    "We will use the function \"optimise_model\" from the file \"model\" to do this. The output will be a table containing different values for the hyper-parameters and a score to show how the variation performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a99194b-6bbe-4c87-8b61-91ef00ab0547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    number of neighbours  best PCA     score\n",
      "0                      1         3  0.895105\n",
      "1                      2         2  0.881119\n",
      "2                      3         5  0.923077\n",
      "3                      4         5  0.937063\n",
      "4                      5         6  0.951049\n",
      "5                      6         2  0.909091\n",
      "6                      7         5  0.958042\n",
      "7                      8         5  0.944056\n",
      "8                      9         5  0.958042\n",
      "9                     10         5  0.972028\n",
      "10                    11         8  0.965035\n",
      "11                    12         9  0.951049\n",
      "12                    13         5  0.951049\n",
      "13                    14         7  0.958042\n",
      "14                    15         6  0.972028\n",
      "15                    16         7  0.958042\n",
      "16                    17         6  0.972028\n",
      "17                    18         8  0.972028\n",
      "18                    19         7  0.965035\n",
      "19                    20         8  0.965035\n",
      "20                    21         4  0.958042\n",
      "21                    22         6  0.965035\n",
      "22                    23         6  0.958042\n",
      "23                    24         6  0.965035\n",
      "24                    25         5  0.958042\n",
      "25                    26         4  0.958042\n",
      "26                    27         4  0.951049\n",
      "27                    28         4  0.951049\n",
      "28                    29         5  0.958042\n",
      "29                    30         6  0.951049\n",
      "30                    31         5  0.951049\n",
      "31                    32         5  0.944056\n",
      "32                    33         4  0.944056\n",
      "33                    34         6  0.951049\n",
      "34                    35         6  0.958042\n",
      "35                    36         5  0.951049\n",
      "36                    37         5  0.944056\n",
      "37                    38         5  0.951049\n",
      "38                    39         4  0.944056\n",
      "39                    40         6  0.958042\n",
      "40                    41         4  0.944056\n",
      "41                    42         5  0.951049\n",
      "42                    43         4  0.944056\n",
      "43                    44         5  0.951049\n",
      "44                    45         8  0.965035\n",
      "45                    46         6  0.951049\n",
      "46                    47         6  0.951049\n",
      "47                    48         6  0.951049\n",
      "48                    49         5  0.951049\n",
      "49                    50         5  0.958042\n"
     ]
    }
   ],
   "source": [
    "import model   #import the file needed (make sure it is in the same directory as the notebook)\n",
    "results = model.optimise_model(train_X_s, train_y_s, test_X_s, test_y_s)  #the test and training data subset are put as arguments\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d5ea9a-e5a8-47b8-90b6-e4c834a32100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The parameters for the optimum model are:\n",
      "number of neighbours    10.000000\n",
      "best PCA                 5.000000\n",
      "score                    0.972028\n",
      "Name: 9, dtype: float64\n",
      " The optimised model is a very good with a score of 0.97\n"
     ]
    }
   ],
   "source": [
    "max_score = results[\"score\"].max() # to get the score for the best performing model\n",
    "index_of_max_score = results[\"score\"].idxmax() # get index of the score\n",
    "parameters_for_optimum = results.loc[index_of_max_score] # obtain the parameters which lead to the score\n",
    "\n",
    "print(f\" The parameters for the optimum model are:\") \n",
    "print(parameters_for_optimum)\n",
    " \n",
    "if max_score >= 0.97:\n",
    "    print(f\" The optimised model is a very good with a score of {max_score:.2f}\")\n",
    "else:\n",
    "    print(f\" The optimised model is not a very good model as it only has a score of {max_score:.2f}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574e365-2cb3-46fe-a5e8-5d05b2a51a37",
   "metadata": {},
   "source": [
    "# Fitting the optimum model \n",
    "This would allow the model we want to be store and be able to be called upon when we use predict later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c1de5a6-53e1-4173-8549-505f923be7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('pca', PCA(n_components=5)),\n",
      "                ('kneighborsclassifier', KNeighborsClassifier(n_neighbors=10))])\n"
     ]
    }
   ],
   "source": [
    "best_pca_parameters = results.loc[index_of_max_score, \"best PCA\"] # returns the value of PCA to use\n",
    "best_neighbours_parameters = results.loc[index_of_max_score, \"number of neighbours\"] #returns the number of neighbours to use\n",
    "\n",
    "#make a pipeline which will scales our data, conducts a PCA and set a number of neighbour to consider\n",
    "optimised_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PCA(n_components = best_pca_parameters),   \n",
    "    KNeighborsClassifier(n_neighbors = best_neighbours_parameters )\n",
    ")\n",
    "\n",
    "fitted_model = optimised_model.fit(train_X_s, train_y_s) # fit our optimised model to the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54219d42-4b01-4d6a-8609-b6eda6a79634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97.5% of our variance is explained by the 5 PCA components included in our model.\n"
     ]
    }
   ],
   "source": [
    "percentage_pca = sum(fitted_model[\"pca\"].explained_variance_ratio_) *100  #percentage reflecting how much of the variance  \n",
    "                                                                          #in the data is explained by each pca component\n",
    "print(f\" {percentage_pca:.1f}% of our variance is explained by the {best_pca_parameters} PCA components included in our model.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c968b7ef-9191-4c95-8884-0eecf6bba098",
   "metadata": {},
   "source": [
    "# Visualising the model...or not\n",
    "Depending on the number of PCA component used by the model, it may not be possible to visualise the model. This is because each PCA becomes an axis in our graph and it's not posible to plot a 1D graph, or any dimension over 4!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8bf1a3d-bff7-4f4f-8279-f3bd5a18f6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As we have 5 PCA components, we are unable to visualise our model as we would need to plot a 5 dimensional graph and that just not possible yet!\n"
     ]
    }
   ],
   "source": [
    "if best_pca_parameters >= 4 or  best_pca_parameters == 1 :\n",
    "    print(f\"As we have {best_pca_parameters} PCA components, we are unable to visualise our model as we would need to plot a {best_pca_parameters} dimensional graph and that just not possible yet!\")\n",
    "elif best_pca_parameters == 2:\n",
    "    from plot import plot_knn # import a function which will plot a 2D graph, make sure the plot file is in the same directory\n",
    "    plot_knn(fitted_model, X_subset, y)\n",
    "else:\n",
    "    print(f\"To plota graph for {best_pca_parameters} PCA components is possible but difficult and this notebook does not deal with this\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab674b0-ab89-49dd-8065-99c99ce365c4",
   "metadata": {},
   "source": [
    "# Making predictions\n",
    "Now we have our model, and we are sufficiently happy with it, we can now input some data in and see what diagnoses is predicted. Have a go inputting some values where indicated. \n",
    "\n",
    "An example  of a random data set is [12.4, 20, 80.9, 470, 0.2, 0.1, 0.007, 0.033, 0.18, 0.072]. When the load_breast_cancer data is used to fit the model, this example data set gives a diagnoses of malignant. \n",
    "\n",
    "Go on play around with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92536e4c-d95a-443b-9079-f2ccd94b612e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 10)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20744\\357590925.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mpredicted_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimised_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpredicted_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m         X = self._validate_data(\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m             \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    803\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    806\u001b[0m                 \u001b[1;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m                 \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 10)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "# Enter data into the new_X DataFrame being created below.\n",
    "#You can put as many values in each list as you want just be consitance across all the variable.\n",
    "\n",
    "new_X = pd.DataFrame({\n",
    "    \"mean radius\": [],   \n",
    "    \"mean texture\": [],\n",
    "    \"mean perimeter\": [],\n",
    "    \"mean area\": [],\n",
    "    \"mean smoothness\": [],\n",
    "    \"mean compactness\": [],\n",
    "    \"mean concavity\": [],\n",
    "    \"mean concave points\": [],\n",
    "    \"mean symmetry\":[],\n",
    "    \"mean fractal dimension\": [],\n",
    "})\n",
    "\n",
    "\n",
    "predicted_y = optimised_model.predict(new_X)\n",
    "\n",
    "for i in predicted_y:\n",
    "    if i == 0:\n",
    "        print(\"Tumour is benign\")\n",
    "    elif i == 1:\n",
    "        print(\"Tumour is malignant\")\n",
    "    else:\n",
    "        print(\"There has been an error, please check data values imputed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
